{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnistdd (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuant9/visual-recognition-MNIST-double-digit-project-/blob/master/mnistdd_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmNeDQOrXznt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.draw import polygon\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import tensorflow as tf\n",
        "from tensorflow import layers as tl\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnf9O5gsXzn7",
        "colab_type": "code",
        "outputId": "1d7b0c06-6fb8-4b57-c595-5dd6a16befe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install -U matplotlib\n",
        "def mkdirs(dirs):\n",
        "  \n",
        "    for d in dirs:\n",
        "        if not os.path.exists(d):\n",
        "            os.mkdir(d)\n",
        "        elif not os.path.isdir(d):\n",
        "            os.remove(d)\n",
        "            os.mkdir(d)\n",
        "train = False\n",
        "mkdirs([\"model\",\"model/detect\", \"model/classify\", \"model/combine\", \"sample\", \"sample/detect\", \"sample/classify\", \"sample/combine\", \"log\", \"log/detect\", \"log/classify\", \"log/combine\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WrYAf5gXzoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_classification_acc(pred, gt):\n",
        "    # pred and gt are both\n",
        "    assert pred.shape == gt.shape\n",
        "    return (pred == gt).astype(int).sum() / gt.size\n",
        "\n",
        "\n",
        "def compute_iou(b_pred, b_gt):\n",
        "    # b_pred: predicted bounding boxes, shape=(n,2,4)\n",
        "    # b_gt: ground truth bounding boxes, shape=(n,2,4)\n",
        "\n",
        "    n = np.shape(b_gt)[0]\n",
        "    L_pred = np.zeros((64, 64))\n",
        "    L_gt = np.zeros((64, 64))\n",
        "    iou = 0.0\n",
        "    for i in range(n):\n",
        "        for b in range(2):\n",
        "            rr, cc = polygon([b_pred[i, b, 0], b_pred[i, b, 0], b_pred[i, b, 2], b_pred[i, b, 2]],\n",
        "                             [b_pred[i, b, 1], b_pred[i, b, 3], b_pred[i, b, 3], b_pred[i, b, 1]], [64, 64])\n",
        "            L_pred[rr, cc] = 1\n",
        "\n",
        "            rr, cc = polygon([b_gt[i, b, 0], b_gt[i, b, 0], b_gt[i, b, 2], b_gt[i, b, 2]],\n",
        "                             [b_gt[i, b, 1], b_gt[i, b, 3], b_gt[i, b, 3], b_gt[i, b, 1]], [64, 64])\n",
        "            L_gt[rr, cc] = 1\n",
        "\n",
        "            iou += (1.0 / (2 * n)) * (np.sum((L_pred + L_gt) == 2) / np.sum((L_pred + L_gt) >= 1))\n",
        "\n",
        "            L_pred[:, :] = 0\n",
        "            L_gt[:, :] = 0\n",
        "\n",
        "    return iou\n",
        "\n",
        "\n",
        "def evaluation(pred_class, pred_bboxes, prefix=\"valid\"):\n",
        "    # pred_class: Your predicted labels for the 2 digits, shape [N, 2]\n",
        "    # pred_bboxes: Your predicted bboxes for 2 digits, shape [N, 2, 4]\n",
        "    gt_class = np.load(prefix + \"_Y.npy\")\n",
        "    gt_bboxes = np.load(prefix + \"_bboxes.npy\")\n",
        "    acc = compute_classification_acc(pred_class, gt_class)\n",
        "    iou = compute_iou(pred_bboxes, gt_bboxes)\n",
        "    print(f\"Classification Acc: {acc}\")\n",
        "    print(f\"BBoxes IOU: {iou}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjXIY4QPXzoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def part_iou(pred, gt):\n",
        "    L_pred = np.zeros((64, 64))\n",
        "    L_gt = np.zeros((64, 64))\n",
        "    rr, cc = polygon([pred[0], pred[0], pred[2], pred[2]],\n",
        "                     [pred[1], pred[3], pred[3], pred[1]], [64, 64])\n",
        "    L_pred[rr, cc] = 1\n",
        "    rr, cc = polygon([gt[0], gt[0], gt[2], gt[2]],\n",
        "                     [gt[1], gt[3], gt[3], gt[1]], [64, 64])\n",
        "    L_gt[rr, cc] = 1\n",
        "    return np.sum((L_pred + L_gt) == 2) / np.sum((L_pred + L_gt) >= 1)\n",
        "\n",
        "def adjust_predict_bboxes(predict_bboxes, bboxes):  # Adjust order according to IOU\n",
        "    assert len(predict_bboxes) == len(bboxes)\n",
        "    n = len(predict_bboxes)\n",
        "    result = np.zeros_like(predict_bboxes)\n",
        "    for i in range(n):\n",
        "        t1 = part_iou(predict_bboxes[i, 0], bboxes[i, 0]) + part_iou(predict_bboxes[i, 0], bboxes[i, 0])\n",
        "        t2 = part_iou(predict_bboxes[i, 0], bboxes[i, 1]) + part_iou(predict_bboxes[i, 1], bboxes[i, 0])\n",
        "        if t1 < t2:\n",
        "            result[i] = predict_bboxes[i, ::-1]\n",
        "        else:\n",
        "            result[i] = predict_bboxes[i]\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_model(ckpt, out):  # Save the model to .npz file\n",
        "    reader = tf.pywrap_tensorflow.NewCheckpointReader(ckpt)\n",
        "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
        "    model = {}\n",
        "    for key in var_to_shape_map.keys():\n",
        "        if \"Adam\" not in key and (\"kernel\" in key or \"bias\" in key):\n",
        "            model[key] = reader.get_tensor(key)\n",
        "    np.savez(out, **model)\n",
        "\n",
        "\n",
        "def crop_image(input_x, input_y, input_bboxes):  # Crop image\n",
        "    assert len(input_x) == len(input_y) and len(input_x) == len(input_bboxes)\n",
        "    size = len(input_x)\n",
        "    output_x = np.zeros([size * 2, 28 * 28], np.uint8)\n",
        "    output_y = input_y.reshape([size * 2]).astype(np.int64)\n",
        "    for i in range(size):\n",
        "        for j in range(2):\n",
        "            output_x[i * 2 + j, :] = input_x[i].reshape((64, 64))[input_bboxes[i, j, 0]:input_bboxes[i, j, 2], input_bboxes[i, j, 1]:input_bboxes[i, j, 3]].flatten()\n",
        "    return output_x, output_y\n",
        "\n",
        "\n",
        "def crop_image_expand_rand_count(input_x, input_y, input_bboxes, pad=4, count=10):  # Random offset cropping image\n",
        "    assert len(input_x) == len(input_y) and len(input_x) == len(input_bboxes)\n",
        "    size = len(input_x)\n",
        "    output_x = np.zeros([size * 2 * count, 28 * 28], np.uint8)\n",
        "    output_y = np.zeros(size * 2 * count, np.int64)\n",
        "    base = np.zeros((64 + pad * 2, 64 + pad * 2), np.uint8)\n",
        "    for i in range(size):\n",
        "        for j in range(2):\n",
        "            base[pad:pad + 64, pad:pad + 64] = input_x[i].reshape((64, 64))\n",
        "            b = np.tile(np.array([np.random.randint(-pad, pad, count), np.random.randint(-pad, pad, count)]).transpose(), 2) + input_bboxes[i, j] + [pad, pad, pad, pad]\n",
        "            for k in range(count):\n",
        "                output_x[(i * 2 + j) * count + k, :] = base[b[k, 0]:b[k, 2], b[k, 1]:b[k, 3]].flatten()\n",
        "                output_y[(i * 2 + j) * count + k] = input_y[i, j]\n",
        "    return output_x, output_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVLatLKpXzoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_classify(pre_model=None, reuse=None):  # Classification model that supports loading parameters from npz files\n",
        "    with tf.variable_scope(\"classify\", reuse=reuse):\n",
        "        x = tf.placeholder(tf.uint8, [None, 28 * 28])\n",
        "        y = tf.placeholder(tf.int64, [None])\n",
        "        h_00 = tf.divide(tf.cast(tf.reshape(x, [-1, 28, 28, 1]), tf.float32), 255.)  # 28x28x1\n",
        "        if pre_model is not None:\n",
        "            h_01 = tl.conv2d(h_00, 16, 5, padding=\"same\", activation=tf.nn.leaky_relu, kernel_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_1/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_1/bias\"]), name=\"conv2d_1\")  # 28x28x16\n",
        "            h_02 = tl.max_pooling2d(h_01, 2, 2)  # 14x14x16\n",
        "            h_03 = tl.conv2d(h_02, 64, 5, padding=\"same\", activation=tf.nn.leaky_relu, kernel_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_2/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_2/bias\"]), name=\"conv2d_2\")  # 14x14x64\n",
        "            h_04 = tl.max_pooling2d(h_03, 2, 2)  # 7x7x64\n",
        "            h_05 = tl.flatten(h_04)\n",
        "            h_06 = tl.dense(h_05, 1024, activation=tf.nn.leaky_relu, kernel_initializer=tf.initializers.constant(pre_model[\"classify/dense_3/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"classify/dense_3/bias\"]), name=\"dense_3\")\n",
        "            dropout_prob = tf.placeholder(tf.float32)\n",
        "            h_07 = tl.dropout(h_06, dropout_prob)\n",
        "            h_08 = tl.dense(h_07, 10, kernel_initializer=tf.initializers.constant(pre_model[\"classify/dense_4/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"classify/dense_4/bias\"]), name=\"dense_4\")\n",
        "        else:\n",
        "            h_01 = tl.conv2d(h_00, 16, 5, padding=\"same\", activation=tf.nn.leaky_relu, name=\"conv2d_1\")  # 28x28x16\n",
        "            h_02 = tl.max_pooling2d(h_01, 2, 2)  # 14x14x16\n",
        "            h_03 = tl.conv2d(h_02, 64, 5, padding=\"same\", activation=tf.nn.leaky_relu, name=\"conv2d_2\")  # 14x14x64\n",
        "            h_04 = tl.max_pooling2d(h_03, 2, 2)  # 7x7x64\n",
        "            h_05 = tl.flatten(h_04)\n",
        "            h_06 = tl.dense(h_05, 1024, activation=tf.nn.leaky_relu, name=\"dense_3\")\n",
        "            dropout_prob = tf.placeholder(tf.float32)\n",
        "            h_07 = tl.dropout(h_06, dropout_prob)\n",
        "            h_08 = tl.dense(h_07, 10, name=\"dense_4\")\n",
        "        predict_label = tf.argmax(h_08, axis=1)\n",
        "        loss_classify = tf.losses.sparse_softmax_cross_entropy(y, h_08)\n",
        "        classify_accuracy = tf.reduce_mean(tf.cast(tf.equal(predict_label, y), tf.float32))\n",
        "    return x, y, dropout_prob, predict_label, loss_classify, classify_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXjFdMJJXzor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classify(start_step=0, restore=False):  # Train the classification model and save the model as an npz file\n",
        "    train_x = np.load(\"train_X.npy\")\n",
        "    train_y = np.load(\"train_Y.npy\")\n",
        "    train_bboxes = np.load(\"train_bboxes.npy\")\n",
        "    valid_x = np.load(\"valid_X.npy\")\n",
        "    valid_y = np.load(\"valid_Y.npy\")\n",
        "    valid_bboxes = np.load(\"valid_bboxes.npy\")\n",
        "\n",
        "    train_x, train_y = crop_image_expand_rand_count(train_x, train_y, train_bboxes, 4, 10)\n",
        "    valid_x, valid_y = crop_image(valid_x, valid_y, valid_bboxes)\n",
        "\n",
        "    m_x, m_y, m_dropout_prob, m_predict_label, m_loss_classify, m_classify_accuracy = model_classify()\n",
        "\n",
        "    tf.summary.scalar(\"loss_classify\", m_loss_classify)\n",
        "    merged_summary_op = tf.summary.merge_all()\n",
        "\n",
        "    classify_global_step = tf.Variable(0, trainable=False)\n",
        "    classify_lr = tf.train.exponential_decay(learning_rate=2e-6, global_step=classify_global_step, decay_steps=2000, decay_rate=0.99)\n",
        "    classify_op = tf.train.AdamOptimizer(classify_lr).minimize(m_loss_classify)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "\n",
        "    if restore:\n",
        "        saver.restore(sess, \"model/classify/model.ckpt-%d\" % start_step)\n",
        "    summary_writer = tf.summary.FileWriter(\"log/classify\", sess.graph)\n",
        "\n",
        "    for step in range(start_step, 100001):\n",
        "        sample = np.random.randint(0, len(train_x), 100)\n",
        "        if step % 100 == 0:\n",
        "            if step % 1000 == 0:\n",
        "                v_predict_label, v_classify_accuracy = sess.run([m_predict_label, m_classify_accuracy], feed_dict={m_x: valid_x, m_y: valid_y})\n",
        "                print(\"step = %5d, classify_accuracy = %f\" % (step, v_classify_accuracy))\n",
        "                image = np.ones((100 * 28 + 101, 100 * 28 + 101), np.uint8) * 255\n",
        "                for i in range(100):\n",
        "                    for j in range(100):\n",
        "                        k = i * 100 + j\n",
        "                        pl = v_predict_label[k]\n",
        "                        img = Image.fromarray(valid_x[k].reshape([28, 28]))\n",
        "                        img = generate_image(img, label=[pl])\n",
        "                        image[i * 29 + 1:i * 29 + 29, j * 29 + 1:j * 29 + 29] = np.array(img)\n",
        "                Image.fromarray(image).save(\"sample/classify/%05d.jpg\" % step)\n",
        "                saver.save(sess, \"model/classify/model.ckpt\", global_step=step)\n",
        "            summary, v_classify_lr, v_loss_classify = sess.run([merged_summary_op, classify_lr, m_loss_classify], feed_dict={m_x: train_x[sample], m_y: train_y[sample], classify_global_step: step})\n",
        "            print(\"step = %5d, lr = %g, loss_classify = %f\" % (step, v_classify_lr, v_loss_classify))\n",
        "            summary_writer.add_summary(summary, step)\n",
        "        sess.run(classify_op, feed_dict={m_x: train_x[sample], m_y: train_y[sample], m_dropout_prob: 0.5, classify_global_step: step})\n",
        "    save_model(\"model/classify/model.ckpt-%d\" % 100000, \"classify.npz\")\n",
        "    sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9gKPdh4Xzo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train:\n",
        "    with tf.variable_scope(\"train_classify\", reuse=tf.AUTO_REUSE):\n",
        "        train_classify(0, False)  # Before running the next training, you need to restart the service, execute the previous function, skip this train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy69IfFuXzpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_detect(pre_model=None, reuse=None):  # Target detection model, applying migration learning, loading some parameters from the classification model\n",
        "    with tf.variable_scope(\"detect\", reuse=reuse):\n",
        "        x = tf.placeholder(tf.uint8, [None, 64 * 64])\n",
        "        bboxes = tf.placeholder(tf.uint8, [None, 2, 4])\n",
        "        center = tf.add(*tf.split(bboxes, 2, 2)) / 2 - 32\n",
        "        h_00 = tf.divide(tf.cast(tf.reshape(x, [-1, 64, 64, 1]), tf.float32), 255.)  # 64x64x1\n",
        "        if pre_model is not None:\n",
        "            h_01 = tl.conv2d(h_00, 16, 5, padding=\"same\", activation=tf.nn.leaky_relu, kernel_initializer=tf.initializers.constant(pre_model[\"detect/conv2d_1/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"detect/conv2d_1/bias\"]), name=\"conv2d_1\")  # 64x64x16\n",
        "            h_02 = tl.max_pooling2d(h_01, 2, 2)  # 32x32x16\n",
        "            h_03 = tl.conv2d(h_02, 64, 5, padding=\"same\", activation=tf.nn.leaky_relu, kernel_initializer=tf.initializers.constant(pre_model[\"detect/conv2d_2/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"detect/conv2d_2/bias\"]), name=\"conv2d_2\")  # 32x32x64\n",
        "            h_04 = tl.max_pooling2d(h_03, 2, 2)  # 16x16x64\n",
        "            h_05 = tl.flatten(h_04)\n",
        "            h_06 = tl.dense(h_05, 4096, activation=tf.nn.tanh, kernel_initializer=tf.initializers.constant(pre_model[\"detect/dense_3/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"detect/dense_3/bias\"]), name=\"dense_3\")\n",
        "            dropout_prob = tf.placeholder(tf.float32)\n",
        "            h_07 = tl.dropout(h_06, dropout_prob)\n",
        "            h_08 = tl.dense(h_07, 4, kernel_initializer=tf.initializers.constant(pre_model[\"detect/dense_4/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"detect/dense_4/bias\"]), name=\"dense_4\")\n",
        "        else:\n",
        "            pre_model = np.load(\"classify.npz\")\n",
        "            h_01 = tl.conv2d(h_00, 16, 5, padding=\"same\", activation=tf.nn.leaky_relu, kernel_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_1/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_1/bias\"]), name=\"conv2d_1\")  # 64x64x16\n",
        "            h_02 = tl.max_pooling2d(h_01, 2, 2)  # 32x32x16\n",
        "            h_03 = tl.conv2d(h_02, 64, 5, padding=\"same\", activation=tf.nn.leaky_relu, kernel_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_2/kernel\"]), bias_initializer=tf.initializers.constant(pre_model[\"classify/conv2d_2/bias\"]), name=\"conv2d_2\")  # 32x32x64\n",
        "            h_04 = tl.max_pooling2d(h_03, 2, 2)  # 16x16x64\n",
        "            h_05 = tl.flatten(h_04)\n",
        "            h_06 = tl.dense(h_05, 4096, activation=tf.nn.tanh, name=\"dense_3\")\n",
        "            dropout_prob = tf.placeholder(tf.float32)\n",
        "            h_07 = tl.dropout(h_06, dropout_prob)\n",
        "            h_08 = tl.dense(h_07, 4, name=\"dense_4\")\n",
        "        predict_center = tf.clip_by_value(tf.reshape(h_08, [-1, 2, 2]), -18, 18)\n",
        "        predict_center_round = tf.round(predict_center)\n",
        "        overlap_h, overlap_v = tf.split(tf.maximum(28 - tf.abs(predict_center_round - center), 0), 2, 2)\n",
        "        overlap = overlap_h * overlap_v\n",
        "        iou = tf.reduce_mean(overlap / (2 * 28 * 28 - overlap))\n",
        "        predict_bboxes = tf.concat([predict_center_round + 32 - 14, predict_center_round + 32 + 14], 2)\n",
        "        loss_detect = tf.losses.absolute_difference(center, predict_center)\n",
        "    return x, bboxes, dropout_prob, predict_bboxes, loss_detect, iou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otRRMvrWXzpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_detect(start_step=0, restore=False):  # Training target detection model\n",
        "    train_x = np.load(\"train_X.npy\")\n",
        "    train_bboxes = np.load(\"train_bboxes.npy\")\n",
        "    valid_x = np.load(\"valid_X.npy\")\n",
        "    valid_bboxes = np.load(\"valid_bboxes.npy\")\n",
        "\n",
        "    m_x, m_bboxes, m_dropout_prob, m_predict_bboxes, m_loss_detect, m_iou = model_detect()\n",
        "\n",
        "    tf.summary.scalar(\"loss_detect\", m_loss_detect)\n",
        "    merged_summary_op = tf.summary.merge_all()\n",
        "\n",
        "    detect_global_step = tf.Variable(0, trainable=False)\n",
        "    detect_lr = tf.train.exponential_decay(learning_rate=0.001, global_step=detect_global_step, decay_steps=100, decay_rate=0.995)\n",
        "    detect_op = tf.train.AdamOptimizer(detect_lr).minimize(m_loss_detect)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "\n",
        "    if restore:\n",
        "        saver.restore(sess, \"model/detect/model.ckpt-%d\" % start_step)\n",
        "    summary_writer = tf.summary.FileWriter(\"log/detect\", sess.graph)\n",
        "\n",
        "    for step in range(start_step, 50001):\n",
        "        sample = np.random.randint(0, len(train_x), 100)\n",
        "        if step % 100 == 0:\n",
        "            if step % 1000 == 0:\n",
        "                v_predict_bboxes, v_iou = sess.run([m_predict_bboxes, m_iou], feed_dict={m_x: valid_x, m_bboxes: valid_bboxes})\n",
        "                iou = compute_iou(valid_bboxes, v_predict_bboxes)\n",
        "                print(\"step = %5d, detect_iou = %f,  iou = %f\" % (step, iou, v_iou))\n",
        "                image = np.ones((100 * 64 + 101, 50 * 64 + 51), np.uint8) * 255\n",
        "                for i in range(100):\n",
        "                    for j in range(50):\n",
        "                        k = i * 50 + j\n",
        "                        pb = v_predict_bboxes[k]\n",
        "                        img = Image.fromarray(valid_x[k].reshape([64, 64]))\n",
        "                        img = generate_image(img, bbox=pb)\n",
        "                        image[i * 65 + 1:i * 65 + 65, j * 65 + 1:j * 65 + 65] = np.array(img)\n",
        "                Image.fromarray(image).save(\"sample/detect/%05d.jpg\" % step)\n",
        "                saver.save(sess, \"model/detect/model.ckpt\", global_step=step)\n",
        "            summary, v_detect_lr, v_loss_detect, v_predict_bboxes, v_iou = sess.run([merged_summary_op, detect_lr, m_loss_detect, m_predict_bboxes, m_iou], feed_dict={m_x: train_x[sample], m_bboxes: train_bboxes[sample], detect_global_step: step})\n",
        "            print(\"step = %5d, lr = %g, loss_detect = %f, iou = %f\" % (step, v_detect_lr, v_loss_detect, v_iou))\n",
        "            summary_writer.add_summary(summary, step)\n",
        "        sess.run(detect_op, feed_dict={m_x: train_x[sample], m_bboxes: train_bboxes[sample], m_dropout_prob: 0.5, detect_global_step: step})\n",
        "    save_model(\"model/detect/model.ckpt-%d\" % 50000, \"detect.npz\")\n",
        "    sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CJg8U0tXzpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train:\n",
        "    with tf.variable_scope(\"train_detect\", reuse=tf.AUTO_REUSE):\n",
        "        train_detect(0, False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpmHkTCQXzqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_combine(start_step=0, restore=False):  # Joint training, fine-tuning classification model\n",
        "    train_x = np.load(\"train_X.npy\")\n",
        "    train_y = np.load(\"train_Y.npy\")\n",
        "    train_bboxes = np.load(\"train_bboxes.npy\")\n",
        "    valid_x = np.load(\"valid_X.npy\")\n",
        "    valid_y = np.load(\"valid_Y.npy\")\n",
        "    valid_bboxes = np.load(\"valid_bboxes.npy\")\n",
        "\n",
        "    md_x, _, _, md_predict_bboxes, _, m_iou = model_detect(np.load(\"model/detect.npz\"), tf.AUTO_REUSE)\n",
        "    mc_x, mc_y, mc_dropout_prob, mc_predict_label, mc_loss_classify, mc_classify_accuracy = model_classify(np.load(\"model/classify.npz\"), tf.AUTO_REUSE)\n",
        "\n",
        "    tf.summary.scalar(\"loss_classify\", mc_loss_classify)\n",
        "    merged_summary_op = tf.summary.merge_all()\n",
        "\n",
        "    classify_global_step = tf.Variable(0, trainable=False)\n",
        "    classify_lr = tf.train.exponential_decay(learning_rate=1e-6, global_step=classify_global_step, decay_steps=1000, decay_rate=0.99)\n",
        "    classify_op = tf.train.AdamOptimizer(classify_lr).minimize(mc_loss_classify)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "    if restore:\n",
        "        saver.restore(sess, \"model/combine/model.ckpt-%d\" % start_step)\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter(\"log\", sess.graph)\n",
        "\n",
        "    for step in range(start_step, 5001):\n",
        "        sample = np.random.randint(0, len(train_x), 100)\n",
        "        vd_predict_bboxes = sess.run(md_predict_bboxes, feed_dict={md_x: train_x[sample]})\n",
        "        vd_predict_bboxes = adjust_predict_bboxes(np.round(vd_predict_bboxes).astype(np.uint8), train_bboxes[sample])\n",
        "        sample_x, sample_y = crop_image(train_x[sample], train_y[sample], vd_predict_bboxes)\n",
        "        if step % 100 == 0:\n",
        "            if step % 1000 == 0:\n",
        "                vd_predict_bboxes = sess.run(md_predict_bboxes, feed_dict={md_x: valid_x})\n",
        "                vd_predict_bboxes = adjust_predict_bboxes(np.round(vd_predict_bboxes).astype(np.uint8), valid_bboxes)\n",
        "                sample_x, sample_y = crop_image(valid_x, valid_y, vd_predict_bboxes)\n",
        "                vc_predict_label, vc_classify_accuracy = sess.run([mc_predict_label, mc_classify_accuracy], feed_dict={mc_x: sample_x, mc_y: sample_y})\n",
        "                vc_predict_label = vc_predict_label.reshape([5000, 2])\n",
        "                image = np.ones((100 * 64 + 101, 50 * 64 + 51), np.uint8) * 255\n",
        "                for i in range(100):\n",
        "                    for j in range(50):\n",
        "                        k = i * 50 + j\n",
        "                        pl = vc_predict_label[k]\n",
        "                        pb = vd_predict_bboxes[k]\n",
        "                        img = Image.fromarray(valid_x[k].reshape([64, 64]))\n",
        "                        img = generate_image(img, label=pl, bbox=pb)\n",
        "                        image[i * 65 + 1:i * 65 + 65, j * 65 + 1:j * 65 + 65] = np.array(img)\n",
        "                Image.fromarray(image).save(\"sample/combine/%05d.jpg\" % step)\n",
        "                evaluation(vc_predict_label, vd_predict_bboxes, \"train_val/valid\")\n",
        "                saver.save(sess, \"model/combine/model.ckpt\", global_step=step)\n",
        "            summary, v_classify_lr, v_loss_classify = sess.run([merged_summary_op, classify_lr, mc_loss_classify], feed_dict={mc_x: sample_x, mc_y: sample_y, classify_global_step: step})\n",
        "            print(\"step = %5d, lr = %g, loss_classify = %f\" % (step, v_classify_lr, v_loss_classify))\n",
        "            summary_writer.add_summary(summary, step)\n",
        "        sess.run(classify_op, feed_dict={mc_x: sample_x, mc_y: sample_y, mc_dropout_prob: 0.5, classify_global_step: step})\n",
        "    save_model(\"model/combine/model.ckpt-%d\" % 5000, \"combine.npz\")\n",
        "    sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybvJjg7mXzqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train:\n",
        "    with tf.variable_scope(\"train_combine\", reuse=tf.AUTO_REUSE):\n",
        "        train_combine(0, False)  # Before running the next testing, you need to restart the service, execute the previous function, skip this train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGs3BRfeXzqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def test_combine():  # Test and output\n",
        "    valid_x = np.load(\"valid_X.npy\")#If you are running the test, please change the file name\n",
        "    \n",
        "    model_data = np.load(\"combine.npz\")\n",
        "    md_x, _, _, md_predict_bboxes, _, m_iou = model_detect(model_data, tf.AUTO_REUSE)\n",
        "    mc_x, mc_y, mc_dropout_prob, mc_predict_label, mc_loss_classify, mc_classify_accuracy = model_classify(model_data, tf.AUTO_REUSE)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    vd_predict_bboxes = sess.run(md_predict_bboxes, feed_dict={md_x: valid_x})\n",
        "    sample_x, _ = crop_image(valid_x, np.zeros((len(valid_x), 2), np.uint8), vd_predict_bboxes.astype(np.uint8))\n",
        "    vc_predict_label = sess.run(mc_predict_label, feed_dict={mc_x: sample_x})\n",
        "    vc_predict_label = vc_predict_label.reshape([5000, 2])  #if you are testing, please chage the argument [5000,2] to [1000,2]\n",
        "   \n",
        "    # if you are runing a test,please change the third arguement \"valid\" to \"test\"\n",
        "    evaluation(vc_predict_label, vd_predict_bboxes, \"valid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IzxOHErXzqp",
        "colab_type": "code",
        "outputId": "21fa1165-e454-4ba6-dbca-a164fba25374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "with tf.variable_scope(\"test_combine\", reuse=tf.AUTO_REUSE):\n",
        "    test_combine()  #to run the evaluation function provided on eclass, just run this function since evaluation is called in this function"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Acc: 0.9724\n",
            "BBoxes IOU: 0.998981012919552\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}